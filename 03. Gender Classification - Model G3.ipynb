{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(7532)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(7532)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Dense, Dropout, Conv2D, Flatten, MaxPool2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train_set_metadata.csv')\n",
    "valid_set = pd.read_csv('valid_set_metadata.csv')\n",
    "test_set = pd.read_csv('test_set_metadata.csv')\n",
    "\n",
    "train_set_partition = np.load('train_set_partition.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of parts the train set was partitioned into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_parts = len(train_set_partition) - 1\n",
    "n_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Training from Scratch with Data Augmentation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_PATH = 'best_model.hdf5'\n",
    "INPUT_SHAPE = (320, 320, 3)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_model(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())  \n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    " \n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(1024, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(5))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 320, 320, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 160, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 160, 160, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 160, 160, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 80, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 40, 40, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 20, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 10, 10, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 8,657,089\n",
      "Trainable params: 8,653,057\n",
      "Non-trainable params: 4,032\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_sequential_model(INPUT_SHAPE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data is divided into 20 parts the network training is performed one part at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, a technical error is causing the training to be stopped after each and every part. Howerver, the trained model does not seem to be affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 20:\n",
      "Epoch 1/100\n",
      "257/257 [==============================] - 167s 651ms/step - loss: 0.2911 - binary_accuracy: 0.8870 - val_loss: 0.2941 - val_binary_accuracy: 0.8791\n",
      "Epoch 2/100\n",
      "257/257 [==============================] - 141s 548ms/step - loss: 0.2845 - binary_accuracy: 0.8874 - val_loss: 0.2927 - val_binary_accuracy: 0.8798\n",
      "Epoch 3/100\n",
      "257/257 [==============================] - 138s 539ms/step - loss: 0.2836 - binary_accuracy: 0.8905 - val_loss: 0.2929 - val_binary_accuracy: 0.8791\n",
      "Epoch 4/100\n",
      "257/257 [==============================] - 138s 538ms/step - loss: 0.2877 - binary_accuracy: 0.8907 - val_loss: 0.2925 - val_binary_accuracy: 0.8795\n",
      "Epoch 5/100\n",
      "257/257 [==============================] - 143s 557ms/step - loss: 0.2857 - binary_accuracy: 0.8912 - val_loss: 0.2928 - val_binary_accuracy: 0.8779\n",
      "Epoch 6/100\n",
      "257/257 [==============================] - 139s 539ms/step - loss: 0.2834 - binary_accuracy: 0.8887 - val_loss: 0.2931 - val_binary_accuracy: 0.8791\n",
      "Epoch 7/100\n",
      "257/257 [==============================] - 139s 541ms/step - loss: 0.2806 - binary_accuracy: 0.8911 - val_loss: 0.2939 - val_binary_accuracy: 0.8812\n",
      "Epoch 8/100\n",
      "257/257 [==============================] - 139s 541ms/step - loss: 0.2843 - binary_accuracy: 0.8894 - val_loss: 0.2935 - val_binary_accuracy: 0.8788\n",
      "Epoch 9/100\n",
      "257/257 [==============================] - 141s 548ms/step - loss: 0.2832 - binary_accuracy: 0.8922 - val_loss: 0.2930 - val_binary_accuracy: 0.8791\n",
      "Epoch 10/100\n",
      "257/257 [==============================] - 141s 549ms/step - loss: 0.2762 - binary_accuracy: 0.8928 - val_loss: 0.2933 - val_binary_accuracy: 0.8788\n",
      "Epoch 11/100\n",
      "257/257 [==============================] - 141s 551ms/step - loss: 0.2818 - binary_accuracy: 0.8939 - val_loss: 0.2932 - val_binary_accuracy: 0.8798\n",
      "Epoch 12/100\n",
      "257/257 [==============================] - 142s 553ms/step - loss: 0.2794 - binary_accuracy: 0.8900 - val_loss: 0.2936 - val_binary_accuracy: 0.8800\n",
      "Epoch 13/100\n",
      "257/257 [==============================] - 144s 560ms/step - loss: 0.2786 - binary_accuracy: 0.8925 - val_loss: 0.2935 - val_binary_accuracy: 0.8786\n",
      "Epoch 14/100\n",
      "257/257 [==============================] - 142s 552ms/step - loss: 0.2796 - binary_accuracy: 0.8908 - val_loss: 0.2938 - val_binary_accuracy: 0.8795\n",
      "Epoch 15/100\n",
      "257/257 [==============================] - 145s 564ms/step - loss: 0.2784 - binary_accuracy: 0.8926 - val_loss: 0.2938 - val_binary_accuracy: 0.8788\n"
     ]
    }
   ],
   "source": [
    "gender_column_position = train_set.columns.get_loc('gender')\n",
    "\n",
    "X_valid = np.load('valid_set_hmgd_arr.npy')\n",
    "y_valid = valid_set['gender'].values\n",
    "\n",
    "batch_limit = train_set_partition[1:] - train_set_partition[:-1]\n",
    "\n",
    "for part in range(n_parts):        \n",
    "    if part < 19:\n",
    "        continue\n",
    "        \n",
    "    print(f'Part {part + 1}:')\n",
    "    \n",
    "    train_filename = 'train_set_hmgd_arr_' + str(part + 1).zfill(2) + '.npy'\n",
    "    subrange = range(train_set_partition[part], train_set_partition[part + 1])    \n",
    "    X_train = np.load(train_filename)\n",
    "    y_train = train_set.iloc[subrange, gender_column_position].values\n",
    "    \n",
    "    # Model initialization/loading\n",
    "    if part:\n",
    "        model = load_model(BEST_MODEL_PATH)\n",
    "    else:\n",
    "        model = create_sequential_model(X_train.shape[1:])\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(optimizer=adam, \n",
    "                      loss='binary_crossentropy', \n",
    "                      metrics=['binary_accuracy'])\n",
    "        \n",
    "        # Initialize callbacks\n",
    "        checkpoint = ModelCheckpoint(BEST_MODEL_PATH, \n",
    "                                     monitor='val_loss', \n",
    "                                     save_best_only=True, \n",
    "                                     save_weights_only=False)\n",
    "        \n",
    "        lr_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                         factor=0.2, \n",
    "                                         patience=5)        \n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                       patience=11)            \n",
    "        \n",
    "        callback_list = [checkpoint, \n",
    "                         lr_reduction, \n",
    "                         early_stopping]\n",
    "    \n",
    "    \n",
    "    data_gen = ImageDataGenerator(rotation_range=20, \n",
    "                                  width_shift_range=0.2, \n",
    "                                  height_shift_range=0.2, \n",
    "                                  horizontal_flip=True)\n",
    "    \n",
    "    steps_per_epoch = int(batch_limit[part] / BATCH_SIZE)\n",
    "    \n",
    "    model.fit_generator(data_gen.flow(X_train, y_train, batch_size=BATCH_SIZE), \n",
    "                        steps_per_epoch=steps_per_epoch, \n",
    "                        epochs=N_EPOCHS,\n",
    "                        callbacks=callback_list, \n",
    "                        validation_data=(X_valid, y_valid), \n",
    "                        workers=4)\n",
    "    \n",
    "    \n",
    "    #free up memory\n",
    "    del X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the test data and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('test_set_hmgd_arr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7621/7621 [==============================] - 36s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3071273387831475, 0.8758693084896995]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('best_model.hdf5')\n",
    "model.evaluate(X_test, test_set['gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
