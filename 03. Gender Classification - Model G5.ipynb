{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "GLOBAL_SEED = 7532\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(GLOBAL_SEED)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(GLOBAL_SEED)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import History \n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Dense, Dropout, Conv2D, Flatten, MaxPool2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train_set_metadata.csv')\n",
    "valid_set = pd.read_csv('valid_set_metadata.csv')\n",
    "test_set = pd.read_csv('test_set_metadata.csv')\n",
    "\n",
    "train_set_partition = np.load('train_set_partition_100_parts.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of parts the train set was partitioned into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_parts = len(train_set_partition) - 1\n",
    "n_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Training from Scratch with Data Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_PATH = 'best_model.hdf5'\n",
    "CURRENT_MODEL_PATH = 'current_model.hdf5'\n",
    "\n",
    "LOSS_SCORES_PATH = 'loss_scores.npy'\n",
    "ACC_SCORES_PATH = 'accuracy_scores.npy'\n",
    "\n",
    "INPUT_SHAPE = (320, 320, 3)\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 4\n",
    "LEARNING_RATE = 0.0001\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_model(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(48, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())  \n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    " \n",
    "    model.add(Conv2D(96, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(rate=0.5, seed=GLOBAL_SEED))\n",
    "    model.add(Dense(units=512, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5, seed=GLOBAL_SEED))\n",
    "    model.add(Dense(units=512, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5, seed=GLOBAL_SEED))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 320, 320, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 160, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 160, 160, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 160, 160, 48)      13872     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 80, 80, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 80, 80, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 80, 80, 64)        27712     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 40, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 40, 40, 96)        55392     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 20, 20, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 20, 20, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 20, 128)       110720    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 10, 10, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 10, 10, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3277312   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 4,046,737\n",
      "Trainable params: 4,045,489\n",
      "Non-trainable params: 1,248\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_sequential_model(INPUT_SHAPE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_scores, accuracy_scores = np.array([]), np.array([])\n",
    "np.save(LOSS_SCORES_PATH, loss_scores)\n",
    "np.save(ACC_SCORES_PATH, accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_column_position = train_set.columns.get_loc('gender')\n",
    "\n",
    "X_valid = np.load('valid_set_hmgd_arr.npy')\n",
    "y_valid = valid_set['gender'].values\n",
    "\n",
    "batch_limit = train_set_partition[1:] - train_set_partition[:-1]\n",
    "loss_scores, accuracy_scores = np.load(LOSS_SCORES_PATH), np.load(ACC_SCORES_PATH)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'Master epoch {epoch + 1}:')\n",
    "    \n",
    "    for part in range(n_parts):  \n",
    "        print(f'Part {part + 1}:')\n",
    "    \n",
    "        train_filename = 'train_set_hmgd_arr_100_parts_' + str(part + 1).zfill(3) + '.npy'\n",
    "        subrange = range(train_set_partition[part], train_set_partition[part + 1])    \n",
    "        X_train = np.load(train_filename)\n",
    "        y_train = train_set.iloc[subrange, gender_column_position].values\n",
    "\n",
    "        # Model initialization/loading\n",
    "        if (epoch > 0) or (part > 0):\n",
    "            model = load_model(CURRENT_MODEL_PATH)            \n",
    "        else:\n",
    "            model = create_sequential_model(X_train.shape[1:])\n",
    "            adam = Adam(lr=LEARNING_RATE)\n",
    "            model.compile(optimizer=adam, loss='binary_crossentropy', \n",
    "                          metrics=['binary_accuracy'])\n",
    "        \n",
    "        \n",
    "        data_gen = ImageDataGenerator(rotation_range=20, \n",
    "                                      width_shift_range=0.2, \n",
    "                                      height_shift_range=0.2, \n",
    "                                      horizontal_flip=True)\n",
    "        \n",
    "        steps_per_epoch = int(batch_limit[part] / BATCH_SIZE)\n",
    "    \n",
    "        history = History()\n",
    "        model.fit_generator(data_gen.flow(X_train, y_train, batch_size=BATCH_SIZE), \n",
    "                            steps_per_epoch=steps_per_epoch, \n",
    "                            epochs=1,\n",
    "                            callbacks=[history],\n",
    "                            workers=4, \n",
    "                            verbose=2)\n",
    "        \n",
    "        model.save(CURRENT_MODEL_PATH)\n",
    "        \n",
    "        #free up memory\n",
    "        del X_train\n",
    "        \n",
    "    epoch_evaluation = model.evaluate(X_valid, valid_set['gender'].values)    \n",
    "    epoch_val_loss = epoch_evaluation[0]\n",
    "    epoch_val_accuracy = epoch_evaluation[1]\n",
    "\n",
    "    print(f'\\nMaster epoch {epoch + 1} validation results:\\n Loss: {epoch_val_loss}, Accuracy: {epoch_val_accuracy}\\n\\n')\n",
    "    \n",
    "    # save best model (if appropriate)\n",
    "    if loss_scores.size:\n",
    "        if epoch_val_loss < loss_scores.min():\n",
    "            model.save(BEST_MODEL_PATH)\n",
    "    else:\n",
    "        model.save(BEST_MODEL_PATH)\n",
    "    \n",
    "    loss_scores = np.append(loss_scores, epoch_val_loss)\n",
    "    accuracy_scores = np.append(accuracy_scores, epoch_val_accuracy)\n",
    "\n",
    "    np.save(LOSS_SCORES_PATH, loss_scores)\n",
    "    np.save(ACC_SCORES_PATH, accuracy_scores)    \n",
    "    \n",
    "    # early stopping  \n",
    "    if loss_scores.size:\n",
    "        if len(loss_scores) - np.argmin(loss_scores) >= EARLY_STOPPING_PATIENCE:\n",
    "            break\n",
    "\n",
    "#free up memory\n",
    "del X_valid          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_scores, accuracy_scores = np.load(LOSS_SCORES_PATH), np.load(ACC_SCORES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.38889226, 0.32604104, 0.32710215, 0.30417137, 0.28552794,\n",
       "        0.28671577, 0.28633918, 0.28537287, 0.28410828, 0.28386664,\n",
       "        0.28120725, 0.27707354, 0.28051974, 0.28511419, 0.28015178,\n",
       "        0.279805  , 0.27498961, 0.27672839, 0.27824121, 0.29120619,\n",
       "        0.28168933]),\n",
       " array([0.84506705, 0.86206897, 0.86039272, 0.87715517, 0.88194444,\n",
       "        0.88338123, 0.88457854, 0.88050766, 0.8829023 , 0.88242337,\n",
       "        0.88314176, 0.88769157, 0.88529693, 0.88505747, 0.88864943,\n",
       "        0.8855364 , 0.88984674, 0.88888889, 0.88864943, 0.88457854,\n",
       "        0.88793103]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_scores, accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the test data and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('test_set_hmgd_arr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7621/7621 [==============================] - 27s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28877010718994983, 0.885316887547566]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(BEST_MODEL_PATH)\n",
    "model.evaluate(X_test, test_set['gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
