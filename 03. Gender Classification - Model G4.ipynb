{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(7532)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(7532)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Dense, Dropout, Conv2D, Flatten, MaxPool2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train_set_metadata.csv')\n",
    "valid_set = pd.read_csv('valid_set_metadata.csv')\n",
    "test_set = pd.read_csv('test_set_metadata.csv')\n",
    "\n",
    "train_set_partition = np.load('train_set_partition.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of parts the train set was partitioned into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_parts = len(train_set_partition) - 1\n",
    "n_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_PATH = 'best_model.hdf5'\n",
    "INPUT_SHAPE = (320, 320, 3)\n",
    "\n",
    "LEARNING_RATE = 0.00005\n",
    "N_EPOCHS = 200\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Model Training with VGG19**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 320, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 320, 320, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 320, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 160, 160, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 160, 160, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 160, 160, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 80, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 80, 80, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 40, 40, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 40, 40, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 20, 20, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 10, 10, 512)       0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg19_model = VGG19(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "vgg19_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the convolutional layers, create the model top and attach it to the VGG19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg19_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_top(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 320, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 320, 320, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 320, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 160, 160, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 160, 160, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 160, 160, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 80, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 80, 80, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 40, 40, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 40, 40, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 20, 20, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 53480449  \n",
      "=================================================================\n",
      "Total params: 73,504,833\n",
      "Trainable params: 53,480,449\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_top = create_model_top(vgg19_model.outputs[0].get_shape().as_list()[1:])\n",
    "model = Model(inputs=vgg19_model.inputs, outputs=model_top(vgg19_model.outputs[0]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the appropriate layers are/are not trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on part 1 to adjust the weights of the model top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1:\n",
      "Epoch 1/200\n",
      "258/258 [==============================] - 304s 1s/step - loss: 1.1522 - binary_accuracy: 0.7570 - val_loss: 0.6438 - val_binary_accuracy: 0.8149\n",
      "Epoch 2/200\n",
      "258/258 [==============================] - 275s 1s/step - loss: 0.7151 - binary_accuracy: 0.7846 - val_loss: 0.5267 - val_binary_accuracy: 0.8269\n",
      "Epoch 3/200\n",
      "258/258 [==============================] - 272s 1s/step - loss: 0.5904 - binary_accuracy: 0.7984 - val_loss: 0.4655 - val_binary_accuracy: 0.8206\n",
      "Epoch 4/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.5256 - binary_accuracy: 0.8106 - val_loss: 0.4308 - val_binary_accuracy: 0.8242\n",
      "Epoch 5/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.4706 - binary_accuracy: 0.8135 - val_loss: 0.4117 - val_binary_accuracy: 0.8367\n",
      "Epoch 6/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.4377 - binary_accuracy: 0.8178 - val_loss: 0.4102 - val_binary_accuracy: 0.8326\n",
      "Epoch 7/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.4082 - binary_accuracy: 0.8332 - val_loss: 0.4031 - val_binary_accuracy: 0.8381\n",
      "Epoch 8/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3966 - binary_accuracy: 0.8339 - val_loss: 0.3989 - val_binary_accuracy: 0.8362\n",
      "Epoch 9/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3970 - binary_accuracy: 0.8321 - val_loss: 0.4033 - val_binary_accuracy: 0.8388\n",
      "Epoch 10/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3782 - binary_accuracy: 0.8448 - val_loss: 0.3966 - val_binary_accuracy: 0.8415\n",
      "Epoch 11/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3727 - binary_accuracy: 0.8419 - val_loss: 0.4013 - val_binary_accuracy: 0.8357\n",
      "Epoch 12/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3767 - binary_accuracy: 0.8404 - val_loss: 0.3947 - val_binary_accuracy: 0.8367\n",
      "Epoch 13/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3628 - binary_accuracy: 0.8444 - val_loss: 0.3921 - val_binary_accuracy: 0.8434\n",
      "Epoch 14/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3548 - binary_accuracy: 0.8481 - val_loss: 0.4065 - val_binary_accuracy: 0.8415\n",
      "Epoch 15/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3557 - binary_accuracy: 0.8503 - val_loss: 0.4189 - val_binary_accuracy: 0.8355\n",
      "Epoch 16/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3476 - binary_accuracy: 0.8548 - val_loss: 0.4010 - val_binary_accuracy: 0.8384\n",
      "Epoch 17/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3544 - binary_accuracy: 0.8539 - val_loss: 0.4046 - val_binary_accuracy: 0.8314\n",
      "Epoch 18/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3328 - binary_accuracy: 0.8643 - val_loss: 0.4156 - val_binary_accuracy: 0.8391\n",
      "Epoch 19/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3284 - binary_accuracy: 0.8592 - val_loss: 0.4069 - val_binary_accuracy: 0.8393\n",
      "Epoch 20/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3198 - binary_accuracy: 0.8663 - val_loss: 0.4041 - val_binary_accuracy: 0.8388\n",
      "Epoch 21/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3230 - binary_accuracy: 0.8642 - val_loss: 0.4050 - val_binary_accuracy: 0.8393\n",
      "Epoch 22/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3154 - binary_accuracy: 0.8716 - val_loss: 0.4092 - val_binary_accuracy: 0.8441\n",
      "Epoch 23/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3051 - binary_accuracy: 0.8701 - val_loss: 0.4084 - val_binary_accuracy: 0.8436\n",
      "Epoch 24/200\n",
      "258/258 [==============================] - 273s 1s/step - loss: 0.3018 - binary_accuracy: 0.8763 - val_loss: 0.4085 - val_binary_accuracy: 0.8439\n"
     ]
    }
   ],
   "source": [
    "age_column_position = train_set.columns.get_loc('gender')\n",
    "\n",
    "X_valid = np.load('valid_set_hmgd_arr_VGG19.npy')\n",
    "y_valid = valid_set['gender'].values\n",
    "\n",
    "batch_limit = train_set_partition[1:] - train_set_partition[:-1]\n",
    "\n",
    "     \n",
    "print('Part 1:')\n",
    "\n",
    "train_filename = 'train_set_hmgd_arr_VGG19_' + str(1).zfill(2) + '.npy'\n",
    "subrange = range(train_set_partition[0], train_set_partition[1])    \n",
    "X_train = np.load(train_filename)\n",
    "y_train = train_set.iloc[subrange, age_column_position].values\n",
    "\n",
    "\n",
    "# Compile model\n",
    "adam = Adam(lr=LEARNING_RATE)\n",
    "model.compile(optimizer=adam, \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "# Initialize callbacks\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_PATH, \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=False)\n",
    "\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                 factor=0.2, \n",
    "                                 patience=5)        \n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=11)            \n",
    "\n",
    "callback_list = [checkpoint, \n",
    "                 lr_reduction, \n",
    "                 early_stopping]\n",
    "\n",
    "\n",
    "data_gen = ImageDataGenerator(rotation_range=20, \n",
    "                              width_shift_range=0.2, \n",
    "                              height_shift_range=0.2, \n",
    "                              horizontal_flip=True)\n",
    "\n",
    "steps_per_epoch = int(batch_limit[0] / BATCH_SIZE)\n",
    "\n",
    "model.fit_generator(data_gen.flow(X_train, y_train, batch_size=BATCH_SIZE), \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=N_EPOCHS,\n",
    "                    callbacks=callback_list, \n",
    "                    validation_data=(X_valid, y_valid), \n",
    "                    workers=4)\n",
    "\n",
    "\n",
    "#free up memory\n",
    "del X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfreeze the high-level convolutional layers of VGG19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 320, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 320, 320, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 320, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 160, 160, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 160, 160, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 160, 160, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 80, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 80, 80, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 40, 40, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 40, 40, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 20, 20, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 53480449  \n",
      "=================================================================\n",
      "Total params: 73,504,833\n",
      "Trainable params: 71,179,265\n",
      "Non-trainable params: 2,325,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(BEST_MODEL_PATH)\n",
    "\n",
    "for layer in model.layers[-11:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.trainable)\n",
    "\n",
    "# compile the model to reflect the above changes\n",
    "adam = Adam(lr=LEARNING_RATE)\n",
    "model.compile(optimizer=adam, \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])    \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data is divided into 20 parts the network training is performed one part at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, a technical error is causing the training to be stopped after each and every part. Howerver, the trained model does not seem to be affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1:\n",
      "Epoch 1/200\n",
      "258/258 [==============================] - 389s 2s/step - loss: 0.3851 - binary_accuracy: 0.8354 - val_loss: 0.3755 - val_binary_accuracy: 0.8551\n",
      "Epoch 2/200\n",
      "258/258 [==============================] - 358s 1s/step - loss: 0.3339 - binary_accuracy: 0.8630 - val_loss: 0.3123 - val_binary_accuracy: 0.8690\n",
      "Epoch 3/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.3077 - binary_accuracy: 0.8775 - val_loss: 0.3168 - val_binary_accuracy: 0.8774\n",
      "Epoch 4/200\n",
      "258/258 [==============================] - 354s 1s/step - loss: 0.2822 - binary_accuracy: 0.8841 - val_loss: 0.3319 - val_binary_accuracy: 0.8681\n",
      "Epoch 5/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.2681 - binary_accuracy: 0.8921 - val_loss: 0.3230 - val_binary_accuracy: 0.8724\n",
      "Epoch 6/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.2718 - binary_accuracy: 0.8896 - val_loss: 0.3190 - val_binary_accuracy: 0.8712\n",
      "Epoch 7/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.2444 - binary_accuracy: 0.9003 - val_loss: 0.3352 - val_binary_accuracy: 0.8788\n",
      "Epoch 8/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.2027 - binary_accuracy: 0.9188 - val_loss: 0.3063 - val_binary_accuracy: 0.8863\n",
      "Epoch 9/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.1789 - binary_accuracy: 0.9289 - val_loss: 0.3581 - val_binary_accuracy: 0.8769\n",
      "Epoch 10/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.1669 - binary_accuracy: 0.9317 - val_loss: 0.3432 - val_binary_accuracy: 0.8772\n",
      "Epoch 11/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.1523 - binary_accuracy: 0.9373 - val_loss: 0.3819 - val_binary_accuracy: 0.8772\n",
      "Epoch 12/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.1479 - binary_accuracy: 0.9418 - val_loss: 0.4301 - val_binary_accuracy: 0.8681\n",
      "Epoch 13/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.1246 - binary_accuracy: 0.9484 - val_loss: 0.4025 - val_binary_accuracy: 0.8776\n",
      "Epoch 14/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.1034 - binary_accuracy: 0.9609 - val_loss: 0.5162 - val_binary_accuracy: 0.8623\n",
      "Epoch 15/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.0945 - binary_accuracy: 0.9641 - val_loss: 0.5325 - val_binary_accuracy: 0.8681\n",
      "Epoch 16/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.0888 - binary_accuracy: 0.9646 - val_loss: 0.5545 - val_binary_accuracy: 0.8673\n",
      "Epoch 17/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.0817 - binary_accuracy: 0.9696 - val_loss: 0.5872 - val_binary_accuracy: 0.8621\n",
      "Epoch 18/200\n",
      "258/258 [==============================] - 356s 1s/step - loss: 0.0795 - binary_accuracy: 0.9697 - val_loss: 0.6260 - val_binary_accuracy: 0.8647\n",
      "Epoch 19/200\n",
      "258/258 [==============================] - 355s 1s/step - loss: 0.0749 - binary_accuracy: 0.9697 - val_loss: 0.6148 - val_binary_accuracy: 0.8630\n"
     ]
    }
   ],
   "source": [
    "age_column_position = train_set.columns.get_loc('gender')\n",
    "\n",
    "X_valid = np.load('valid_set_hmgd_arr_VGG19.npy')\n",
    "y_valid = valid_set['gender'].values\n",
    "\n",
    "batch_limit = train_set_partition[1:] - train_set_partition[:-1]\n",
    "\n",
    "for part in range(n_parts):        \n",
    "    print(f'Part {part + 1}:')\n",
    "    \n",
    "    train_filename = 'train_set_hmgd_arr_VGG19_' + str(part + 1).zfill(2) + '.npy'\n",
    "    subrange = range(train_set_partition[part], train_set_partition[part + 1])    \n",
    "    X_train = np.load(train_filename)\n",
    "    y_train = train_set.iloc[subrange, age_column_position].values\n",
    "       \n",
    "    # Initialize callbacks\n",
    "    checkpoint = ModelCheckpoint(BEST_MODEL_PATH, \n",
    "                                 monitor='val_loss', \n",
    "                                 save_best_only=True, \n",
    "                                 save_weights_only=False)\n",
    "\n",
    "    lr_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                     factor=0.2, \n",
    "                                     patience=5)        \n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   patience=11)            \n",
    "\n",
    "    callback_list = [checkpoint, \n",
    "                     lr_reduction, \n",
    "                     early_stopping]\n",
    "\n",
    "    \n",
    "    data_gen = ImageDataGenerator(rotation_range=20, \n",
    "                                  width_shift_range=0.2, \n",
    "                                  height_shift_range=0.2, \n",
    "                                  horizontal_flip=True)\n",
    "    \n",
    "    steps_per_epoch = int(batch_limit[part] / BATCH_SIZE)\n",
    "    \n",
    "    model.fit_generator(data_gen.flow(X_train, y_train, batch_size=BATCH_SIZE), \n",
    "                        steps_per_epoch=steps_per_epoch, \n",
    "                        epochs=N_EPOCHS,\n",
    "                        callbacks=callback_list, \n",
    "                        validation_data=(X_valid, y_valid), \n",
    "                        workers=4)\n",
    "    \n",
    "    \n",
    "    #free up memory\n",
    "    del X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we no longer need to unfreze any layers, we can use the code below to finish the training on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 320, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 320, 320, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 320, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 160, 160, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 160, 160, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 160, 160, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 80, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 80, 80, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 80, 80, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 40, 40, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 40, 40, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 40, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 20, 20, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 53480449  \n",
      "=================================================================\n",
      "Total params: 73,504,833\n",
      "Trainable params: 71,179,265\n",
      "Non-trainable params: 2,325,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(BEST_MODEL_PATH)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 20:\n",
      "Epoch 1/200\n",
      "257/257 [==============================] - 385s 1s/step - loss: 0.2521 - binary_accuracy: 0.9044 - val_loss: 0.2675 - val_binary_accuracy: 0.8927\n",
      "Epoch 2/200\n",
      "257/257 [==============================] - 359s 1s/step - loss: 0.2398 - binary_accuracy: 0.9080 - val_loss: 0.2634 - val_binary_accuracy: 0.8956\n",
      "Epoch 3/200\n",
      "257/257 [==============================] - 355s 1s/step - loss: 0.2322 - binary_accuracy: 0.9110 - val_loss: 0.2847 - val_binary_accuracy: 0.8937\n",
      "Epoch 4/200\n",
      "257/257 [==============================] - 354s 1s/step - loss: 0.2210 - binary_accuracy: 0.9163 - val_loss: 0.2781 - val_binary_accuracy: 0.8906\n",
      "Epoch 5/200\n",
      "257/257 [==============================] - 354s 1s/step - loss: 0.2111 - binary_accuracy: 0.9197 - val_loss: 0.2950 - val_binary_accuracy: 0.8903\n",
      "Epoch 6/200\n",
      "257/257 [==============================] - 354s 1s/step - loss: 0.2026 - binary_accuracy: 0.9212 - val_loss: 0.3134 - val_binary_accuracy: 0.8894\n",
      "Epoch 7/200\n",
      "257/257 [==============================] - 355s 1s/step - loss: 0.1899 - binary_accuracy: 0.9246 - val_loss: 0.2982 - val_binary_accuracy: 0.8870\n",
      "Epoch 8/200\n",
      "257/257 [==============================] - 354s 1s/step - loss: 0.1703 - binary_accuracy: 0.9333 - val_loss: 0.3470 - val_binary_accuracy: 0.8894\n",
      "Epoch 9/200\n",
      "257/257 [==============================] - 354s 1s/step - loss: 0.1630 - binary_accuracy: 0.9346 - val_loss: 0.3673 - val_binary_accuracy: 0.8889\n",
      "Epoch 10/200\n",
      "257/257 [==============================] - 355s 1s/step - loss: 0.1526 - binary_accuracy: 0.9404 - val_loss: 0.3540 - val_binary_accuracy: 0.8882\n",
      "Epoch 11/200\n",
      "257/257 [==============================] - 354s 1s/step - loss: 0.1541 - binary_accuracy: 0.9394 - val_loss: 0.3862 - val_binary_accuracy: 0.8889\n",
      "Epoch 12/200\n",
      "257/257 [==============================] - 355s 1s/step - loss: 0.1473 - binary_accuracy: 0.9429 - val_loss: 0.3943 - val_binary_accuracy: 0.8875\n",
      "Epoch 13/200\n",
      "257/257 [==============================] - 355s 1s/step - loss: 0.1410 - binary_accuracy: 0.9443 - val_loss: 0.3959 - val_binary_accuracy: 0.8860\n"
     ]
    }
   ],
   "source": [
    "age_column_position = train_set.columns.get_loc('gender')\n",
    "\n",
    "X_valid = np.load('valid_set_hmgd_arr_VGG19.npy')\n",
    "y_valid = valid_set['gender'].values\n",
    "\n",
    "batch_limit = train_set_partition[1:] - train_set_partition[:-1]\n",
    "\n",
    "for part in range(n_parts):        \n",
    "    if part < 19:\n",
    "        continue\n",
    "        \n",
    "    print(f'Part {part + 1}:')\n",
    "    \n",
    "    train_filename = 'train_set_hmgd_arr_VGG19_' + str(part + 1).zfill(2) + '.npy'\n",
    "    subrange = range(train_set_partition[part], train_set_partition[part + 1])    \n",
    "    X_train = np.load(train_filename)\n",
    "    y_train = train_set.iloc[subrange, age_column_position].values\n",
    "    \n",
    "\n",
    "    # Initialize callbacks\n",
    "    checkpoint = ModelCheckpoint(BEST_MODEL_PATH, \n",
    "                                 monitor='val_loss', \n",
    "                                 save_best_only=True, \n",
    "                                 save_weights_only=False)\n",
    "\n",
    "    lr_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                     factor=0.2, \n",
    "                                     patience=5)        \n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   patience=11)            \n",
    "\n",
    "    callback_list = [checkpoint, \n",
    "                     lr_reduction, \n",
    "                     early_stopping]\n",
    "\n",
    "    \n",
    "    data_gen = ImageDataGenerator(rotation_range=20, \n",
    "                                  width_shift_range=0.2, \n",
    "                                  height_shift_range=0.2, \n",
    "                                  horizontal_flip=True)\n",
    "    \n",
    "    steps_per_epoch = int(batch_limit[part] / BATCH_SIZE)\n",
    "    \n",
    "    model.fit_generator(data_gen.flow(X_train, y_train, batch_size=BATCH_SIZE), \n",
    "                        steps_per_epoch=steps_per_epoch, \n",
    "                        epochs=N_EPOCHS,\n",
    "                        callbacks=callback_list, \n",
    "                        validation_data=(X_valid, y_valid), \n",
    "                        workers=4)\n",
    "    \n",
    "    \n",
    "    #free up memory\n",
    "    del X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model was trained on 20 parts and was saved after each part we have in fact 20 partial models at our disposal. <br>Hence, we can select the one that produces the smallest loss on the validation set among, say, the last 5 partial models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.load('valid_set_hmgd_arr_VGG19.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4176/4176 [==============================] - 96s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26738093045479494, 0.8924808429118773]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('best_model_VGG19_gender_16.hdf5')\n",
    "model.evaluate(X_valid, valid_set['gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4176/4176 [==============================] - 88s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2711003300844481, 0.8900862068965517]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('best_model_VGG19_gender_17.hdf5')\n",
    "model.evaluate(X_valid, valid_set['gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4176/4176 [==============================] - 88s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2630035652283051, 0.8934386973180076]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('best_model_VGG19_gender_18.hdf5')\n",
    "model.evaluate(X_valid, valid_set['gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4176/4176 [==============================] - 103s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2606325297405893, 0.8958333333333334]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('best_model_VGG19_gender_19.hdf5')\n",
    "model.evaluate(X_valid, valid_set['gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4176/4176 [==============================] - 103s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2634004768968999, 0.8955938697318008]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('best_model_VGG19_gender_20.hdf5')\n",
    "model.evaluate(X_valid, valid_set['gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del X_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the best model is the 19th one. Now, we can read in the test data and check how this model performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('test_set_hmgd_arr_VGG19.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7621/7621 [==============================] - 190s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27547234134706294, 0.8893845951974807]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('best_model_VGG19_gender_19.hdf5')\n",
    "model.evaluate(X_test, test_set['gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
