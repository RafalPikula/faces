{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "GLOBAL_SEED = 7532\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(GLOBAL_SEED)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(GLOBAL_SEED)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.callbacks import Callback, EarlyStopping, History, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Dense, Dropout, Conv2D, Flatten, MaxPool2D, SpatialDropout2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train_set_metadata_HMGD.csv')\n",
    "valid_set = pd.read_csv('valid_set_metadata_HMGD.csv')\n",
    "test_set = pd.read_csv('test_set_metadata_HMGD.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (320, 320, 3)\n",
    "\n",
    "LEARNING_RATE = 0.0002\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the CNN model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=3, activation='elu',padding='same', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(rate=0.25))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(rate=0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='elu', padding='same'))    \n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())  \n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(rate=0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    " \n",
    "    model.add(Conv2D(256, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(rate=0.25))\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(rate=0.25))\n",
    "    model.add(Conv2D(512, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(1024, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(rate=0.25))\n",
    "    model.add(Conv2D(1024, kernel_size=3, activation='elu', padding='same'))\n",
    "    model.add(MaxPool2D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(rate=0.5, seed=GLOBAL_SEED))\n",
    "    model.add(Dense(units=512, activation='elu'))\n",
    "    model.add(Dropout(rate=0.5, seed=GLOBAL_SEED))\n",
    "    model.add(Dense(units=256, activation='elu'))\n",
    "    model.add(Dropout(rate=0.5, seed=GLOBAL_SEED))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 320, 320, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 320, 320, 32)      128       \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_1 (Spatial (None, 320, 320, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 320, 320, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 160, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 160, 160, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 160, 160, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 160, 160, 64)      256       \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_2 (Spatial (None, 160, 160, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 160, 160, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 80, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 80, 80, 128)       512       \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_3 (Spatial (None, 80, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 80, 80, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 40, 40, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 40, 40, 256)       1024      \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_4 (Spatial (None, 40, 40, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 40, 40, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 20, 20, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 20, 20, 512)       2048      \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_5 (Spatial (None, 20, 20, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 20, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 10, 10, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 10, 10, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_6 (Spatial (None, 10, 10, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 10, 10, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               13107712  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 32,125,473\n",
      "Trainable params: 32,117,409\n",
      "Non-trainable params: 8,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(INPUT_SHAPE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the class weights to be used during model training in order to mitigate the class imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0949726587842759, 1: 0.9201873755187876}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_values = train_set['gender'].values\n",
    "classes = np.unique(class_values)\n",
    "\n",
    "weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes, \n",
    "    class_values\n",
    ")\n",
    "class_weights = dict(zip(classes, weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 165400 images.\n",
      "Found 4176 images.\n",
      "Epoch 1/50\n",
      "5168/5168 [==============================] - 5105s 988ms/step - loss: 0.7460 - binary_accuracy: 0.6624 - val_loss: 0.4931 - val_binary_accuracy: 0.7712\n",
      "Epoch 2/50\n",
      "5168/5168 [==============================] - 5112s 989ms/step - loss: 0.4057 - binary_accuracy: 0.8231 - val_loss: 0.3572 - val_binary_accuracy: 0.8485\n",
      "Epoch 3/50\n",
      "5168/5168 [==============================] - 5166s 1000ms/step - loss: 0.3277 - binary_accuracy: 0.8683 - val_loss: 0.3176 - val_binary_accuracy: 0.8728\n",
      "Epoch 4/50\n",
      "5168/5168 [==============================] - 5267s 1s/step - loss: 0.3069 - binary_accuracy: 0.8785 - val_loss: 0.3214 - val_binary_accuracy: 0.8748\n",
      "Epoch 5/50\n",
      "5168/5168 [==============================] - 5245s 1s/step - loss: 0.2982 - binary_accuracy: 0.8840 - val_loss: 0.2998 - val_binary_accuracy: 0.8760\n",
      "Epoch 6/50\n",
      "5168/5168 [==============================] - 5195s 1s/step - loss: 0.2906 - binary_accuracy: 0.8874 - val_loss: 0.2946 - val_binary_accuracy: 0.8769\n",
      "Epoch 7/50\n",
      "5168/5168 [==============================] - 5212s 1s/step - loss: 0.2857 - binary_accuracy: 0.8900 - val_loss: 0.2821 - val_binary_accuracy: 0.8873\n",
      "Epoch 8/50\n",
      "5168/5168 [==============================] - 5142s 995ms/step - loss: 0.2813 - binary_accuracy: 0.8921 - val_loss: 0.2966 - val_binary_accuracy: 0.8825\n",
      "Epoch 9/50\n",
      "5168/5168 [==============================] - 5102s 987ms/step - loss: 0.2775 - binary_accuracy: 0.8941 - val_loss: 0.3187 - val_binary_accuracy: 0.8772\n",
      "Epoch 10/50\n",
      "5168/5168 [==============================] - 5102s 987ms/step - loss: 0.2748 - binary_accuracy: 0.8952 - val_loss: 0.2880 - val_binary_accuracy: 0.8875\n",
      "Epoch 11/50\n",
      "5168/5168 [==============================] - 4875s 943ms/step - loss: 0.2729 - binary_accuracy: 0.8964 - val_loss: 0.3092 - val_binary_accuracy: 0.8738\n",
      "Epoch 12/50\n",
      "5168/5168 [==============================] - 4739s 917ms/step - loss: 0.2620 - binary_accuracy: 0.9001 - val_loss: 0.2874 - val_binary_accuracy: 0.8837\n",
      "Epoch 13/50\n",
      "5168/5168 [==============================] - 4735s 916ms/step - loss: 0.2580 - binary_accuracy: 0.9018 - val_loss: 0.2728 - val_binary_accuracy: 0.8912\n",
      "Epoch 14/50\n",
      "5168/5168 [==============================] - 4733s 916ms/step - loss: 0.2560 - binary_accuracy: 0.9026 - val_loss: 0.2857 - val_binary_accuracy: 0.8834\n",
      "Epoch 15/50\n",
      "5168/5168 [==============================] - 4737s 917ms/step - loss: 0.2553 - binary_accuracy: 0.9029 - val_loss: 0.2742 - val_binary_accuracy: 0.8950\n",
      "Epoch 16/50\n",
      "5168/5168 [==============================] - 4736s 916ms/step - loss: 0.2538 - binary_accuracy: 0.9035 - val_loss: 0.2712 - val_binary_accuracy: 0.8948\n",
      "Epoch 17/50\n",
      "5168/5168 [==============================] - 4751s 919ms/step - loss: 0.2526 - binary_accuracy: 0.9040 - val_loss: 0.2724 - val_binary_accuracy: 0.8941\n",
      "Epoch 18/50\n",
      "5168/5168 [==============================] - 4756s 920ms/step - loss: 0.2521 - binary_accuracy: 0.9044 - val_loss: 0.2716 - val_binary_accuracy: 0.8931\n",
      "Epoch 19/50\n",
      "5168/5168 [==============================] - 4753s 920ms/step - loss: 0.2505 - binary_accuracy: 0.9048 - val_loss: 0.2649 - val_binary_accuracy: 0.8989\n",
      "Epoch 20/50\n",
      "5168/5168 [==============================] - 4762s 921ms/step - loss: 0.2490 - binary_accuracy: 0.9052 - val_loss: 0.2678 - val_binary_accuracy: 0.8919\n",
      "Epoch 21/50\n",
      "5168/5168 [==============================] - 4757s 921ms/step - loss: 0.2484 - binary_accuracy: 0.9058 - val_loss: 0.2659 - val_binary_accuracy: 0.8960\n",
      "Epoch 22/50\n",
      "5168/5168 [==============================] - 4761s 921ms/step - loss: 0.2471 - binary_accuracy: 0.9059 - val_loss: 0.2713 - val_binary_accuracy: 0.8931\n",
      "Epoch 23/50\n",
      "5168/5168 [==============================] - 4767s 922ms/step - loss: 0.2463 - binary_accuracy: 0.9061 - val_loss: 0.2646 - val_binary_accuracy: 0.8955\n",
      "Epoch 24/50\n",
      "5168/5168 [==============================] - 4754s 920ms/step - loss: 0.2453 - binary_accuracy: 0.9066 - val_loss: 0.2638 - val_binary_accuracy: 0.8996\n",
      "Epoch 25/50\n",
      "5168/5168 [==============================] - 4757s 921ms/step - loss: 0.2452 - binary_accuracy: 0.9066 - val_loss: 0.2733 - val_binary_accuracy: 0.8883\n",
      "Epoch 26/50\n",
      "5168/5168 [==============================] - 4760s 921ms/step - loss: 0.2446 - binary_accuracy: 0.9066 - val_loss: 0.2717 - val_binary_accuracy: 0.8979\n",
      "Epoch 27/50\n",
      "5168/5168 [==============================] - 4758s 921ms/step - loss: 0.2434 - binary_accuracy: 0.9065 - val_loss: 0.2639 - val_binary_accuracy: 0.8972\n",
      "Epoch 28/50\n",
      "5168/5168 [==============================] - 4758s 921ms/step - loss: 0.2435 - binary_accuracy: 0.9069 - val_loss: 0.2691 - val_binary_accuracy: 0.8933\n",
      "Epoch 29/50\n",
      "5168/5168 [==============================] - 4761s 921ms/step - loss: 0.2378 - binary_accuracy: 0.9090 - val_loss: 0.2680 - val_binary_accuracy: 0.9015\n",
      "Epoch 30/50\n",
      "5168/5168 [==============================] - 4764s 922ms/step - loss: 0.2359 - binary_accuracy: 0.9089 - val_loss: 0.2695 - val_binary_accuracy: 0.8950\n",
      "Epoch 31/50\n",
      "5168/5168 [==============================] - 4757s 921ms/step - loss: 0.2351 - binary_accuracy: 0.9101 - val_loss: 0.2714 - val_binary_accuracy: 0.8965\n",
      "Epoch 32/50\n",
      "5168/5168 [==============================] - 4755s 920ms/step - loss: 0.2345 - binary_accuracy: 0.9101 - val_loss: 0.2692 - val_binary_accuracy: 0.8921\n",
      "Epoch 33/50\n",
      "5168/5168 [==============================] - 4751s 919ms/step - loss: 0.2317 - binary_accuracy: 0.9105 - val_loss: 0.2769 - val_binary_accuracy: 0.8919\n",
      "Epoch 34/50\n",
      "5168/5168 [==============================] - 4754s 920ms/step - loss: 0.2311 - binary_accuracy: 0.9108 - val_loss: 0.2666 - val_binary_accuracy: 0.9006\n"
     ]
    }
   ],
   "source": [
    "# Create and compile the model\n",
    "adam = Adam(\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=adam, \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "\n",
    "# Initialize callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model_custom_big_epoch_{epoch:03d}.hdf5', \n",
    "    monitor='val_loss', \n",
    "    save_best_only=False, \n",
    "    save_weights_only=False\n",
    ")\n",
    "lr_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=4\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10\n",
    ")    \n",
    "callback_list = [checkpoint, lr_reduction, early_stopping]\n",
    "\n",
    "# generate and augment training and validation data\n",
    "data_generator = ImageDataGenerator(\n",
    "    rotation_range=20, \n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2, \n",
    "    horizontal_flip=True  \n",
    ")\n",
    "train_data_generator = data_generator.flow_from_dataframe(\n",
    "    dataframe=train_set, \n",
    "    directory='imdb_crop/_all_photos/',\n",
    "    x_col='photo_path',\n",
    "    y_col='gender',\n",
    "    target_size=(320, 320),\n",
    "    class_mode='other',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=GLOBAL_SEED\n",
    ")\n",
    "valid_data_generator = data_generator.flow_from_dataframe(\n",
    "    dataframe=valid_set, \n",
    "    directory='imdb_crop/_all_photos/',\n",
    "    x_col='photo_path',\n",
    "    y_col='gender',\n",
    "    target_size=(320, 320),\n",
    "    class_mode='other',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=GLOBAL_SEED\n",
    ")\n",
    "\n",
    "train_steps_per_epoch = train_data_generator.n//train_data_generator.batch_size\n",
    "valid_steps_per_epoch = valid_data_generator.n//valid_data_generator.batch_size\n",
    "\n",
    "#fit the model\n",
    "history = model.fit_generator(\n",
    "    generator=train_data_generator,\n",
    "    steps_per_epoch=train_steps_per_epoch, \n",
    "    epochs=N_EPOCHS,\n",
    "    callbacks=callback_list,\n",
    "    validation_data=valid_data_generator,\n",
    "    validation_steps=valid_steps_per_epoch,\n",
    "    class_weight=class_weights,\n",
    "    workers=4, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7621 images.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27182091733433256, 0.8933210864715917]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_data_generator = ImageDataGenerator().flow_from_dataframe(\n",
    "    dataframe=test_set, \n",
    "    directory='imdb_crop/_all_photos/',\n",
    "    x_col='photo_path',\n",
    "    y_col='gender',\n",
    "    target_size=(320, 320),\n",
    "    class_mode='other',\n",
    "    batch_size=1,\n",
    "    seed=GLOBAL_SEED,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "best_model = load_model('model_custom_big_epoch_029.hdf5')\n",
    "best_model.evaluate_generator(generator=evaluation_data_generator, steps=len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7621 images.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26853205023805626, 0.8947644666054324]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_data_generator = ImageDataGenerator().flow_from_dataframe(\n",
    "    dataframe=test_set, \n",
    "    directory='imdb_crop/_all_photos/',\n",
    "    x_col='photo_path',\n",
    "    y_col='gender',\n",
    "    target_size=(320, 320),\n",
    "    class_mode='other',\n",
    "    batch_size=1,\n",
    "    seed=GLOBAL_SEED,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "best_model = load_model('model_custom_big_epoch_034.hdf5')\n",
    "best_model.evaluate_generator(generator=evaluation_data_generator, steps=len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
